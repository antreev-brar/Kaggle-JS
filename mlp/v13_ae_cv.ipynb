{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.046335,
     "end_time": "2020-12-23T13:07:57.199656",
     "exception": false,
     "start_time": "2020-12-23T13:07:57.153321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "from random import choices\n",
    "\n",
    "import kerastuner as kt\n",
    "        \n",
    "TRAINING = True\n",
    "USE_FINETUNE = False     \n",
    "FOLDS = 4\n",
    "SEED = 42\n",
    "CACHE_PATH = 'v13_ae_cv'\n",
    "try:\n",
    "    os.mkdir(CACHE_PATH)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    train = pd.read_feather('/storage1/lu/Active/tianyang/Workspace/janestreet/train.feather')\n",
    "    train = train.query('date > 85').reset_index(drop = True) \n",
    "    train = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns})\n",
    "    train.fillna(train.mean(),inplace=True)\n",
    "    train = train.query('weight > 0').reset_index(drop = True)\n",
    "\n",
    "    train['action'] = ((train['resp_1'] > 0 ) &\\\n",
    "                       (train['resp_2'] > 0 ) &\\\n",
    "                       (train['resp_3'] > 0 ) &\\\n",
    "                       (train['resp_4'] > 0 ) &\\\n",
    "                       (train['resp'] > 0 )).astype('int')\n",
    "\n",
    "    features = [c for c in train.columns if 'feature' in c]\n",
    "    resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n",
    "\n",
    "    X = train[features].values\n",
    "    y = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T\n",
    "\n",
    "    f_mean = np.mean(train[features[1:]].values, axis=0)\n",
    "    np.save(f'{CACHE_PATH}/f_mean_online.npy', f_mean)\n",
    "    \n",
    "    date_index = train['date'].values\n",
    "    \n",
    "    return X, y, date_index\n",
    "\n",
    "X, y, date_index = preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017036,
     "end_time": "2020-12-23T13:08:03.091538",
     "exception": false,
     "start_time": "2020-12-23T13:08:03.074502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PurgedGroupTimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "papermill": {
     "duration": 0.058881,
     "end_time": "2020-12-23T13:08:03.167446",
     "exception": false,
     "start_time": "2020-12-23T13:08:03.108565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01299,
     "end_time": "2020-12-23T13:10:32.381467",
     "exception": false,
     "start_time": "2020-12-23T13:10:32.368477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Creating the autoencoder. \n",
    "The autoencoder should aid in denoising the data. Based on [this](https://www.semanticscholar.org/paper/Deep-Bottleneck-Classifiers-in-Supervised-Dimension-Parviainen/fb86483f7573f6430fe4597432b0cd3e34b16e43) paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.026674,
     "end_time": "2020-12-23T13:10:32.421414",
     "exception": false,
     "start_time": "2020-12-23T13:10:32.39474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_autoencoder(input_dim, output_dim, noise=0.05):\n",
    "    i = Input(input_dim)\n",
    "    encoded = BatchNormalization()(i)\n",
    "    encoded = GaussianNoise(noise)(encoded)\n",
    "    encoded = Dense(64,activation='relu')(encoded)\n",
    "    decoded = Dropout(0.15)(encoded)\n",
    "    decoded = BatchNormalization()(decoded)\n",
    "    decoded = Dense(input_dim,name='decoded')(decoded)\n",
    "    x = Dense(32, activation='relu')(decoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)    \n",
    "    x = Dense(output_dim, activation='sigmoid' ,name='label_output')(x)\n",
    "    \n",
    "    encoder = Model(inputs=i, outputs=encoded)\n",
    "    autoencoder = Model(inputs=i, outputs=[decoded,x])\n",
    "    \n",
    "    autoencoder.compile(optimizer=Adam(0.001), \n",
    "                        loss={'decoded':'mse', 'label_output':'binary_crossentropy'})\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014228,
     "end_time": "2020-12-23T13:10:32.448917",
     "exception": false,
     "start_time": "2020-12-23T13:10:32.434689",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Creating the MLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "papermill": {
     "duration": 0.02781,
     "end_time": "2020-12-23T13:10:32.490385",
     "exception": false,
     "start_time": "2020-12-23T13:10:32.462575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(hp,input_dim,output_dim,encoder):\n",
    "    inputs = Input(input_dim)\n",
    "    \n",
    "    x = encoder(inputs)\n",
    "    x = Concatenate()([x,inputs, x]) #use both raw and encoded features\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(hp.Float('init_dropout',0.0,0.5))(x)\n",
    "    \n",
    "    for i in range(hp.Int('num_layers',1,3)):\n",
    "        x = Dense(hp.Int('num_units_{i}',64,256))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Lambda(tf.keras.activations.swish)(x)\n",
    "        x = Dropout(hp.Float(f'dropout_{i}',0.0,0.5))(x)\n",
    "        \n",
    "    x = Dense(output_dim, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    # previously 1e-5, 1e-4, 1e-3\n",
    "    model.compile(optimizer=Adam(hp.Float('lr',0.00005,0.005,default=0.001)),\n",
    "                  loss=BinaryCrossentropy(label_smoothing=hp.Float('label_smoothing',0.0,0.1)),\n",
    "                  metrics=[tf.keras.metrics.AUC(name = 'auc')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013658,
     "end_time": "2020-12-23T13:10:32.517883",
     "exception": false,
     "start_time": "2020-12-23T13:10:32.504225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Defining and training the autoencoder. \n",
    "We add gaussian noise with mean and std from training data. After training we lock the layers in the encoder from further training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 2.820062,
     "end_time": "2020-12-23T13:10:35.351534",
     "exception": false,
     "start_time": "2020-12-23T13:10:32.531472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "346/346 [==============================] - 9s 25ms/step - loss: 2.7184 - decoded_loss: 1.9968 - label_output_loss: 0.7216 - val_loss: 1.3251 - val_decoded_loss: 0.6339 - val_label_output_loss: 0.6912\n",
      "Epoch 2/1000\n",
      "346/346 [==============================] - 8s 23ms/step - loss: 1.7248 - decoded_loss: 1.0294 - label_output_loss: 0.6953 - val_loss: 1.1398 - val_decoded_loss: 0.4494 - val_label_output_loss: 0.6904\n",
      "Epoch 3/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.6396 - decoded_loss: 0.9477 - label_output_loss: 0.6919 - val_loss: 1.0896 - val_decoded_loss: 0.3997 - val_label_output_loss: 0.6899\n",
      "Epoch 4/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.6041 - decoded_loss: 0.9130 - label_output_loss: 0.6911 - val_loss: 1.0631 - val_decoded_loss: 0.3737 - val_label_output_loss: 0.6894\n",
      "Epoch 5/1000\n",
      "346/346 [==============================] - 7s 22ms/step - loss: 1.5849 - decoded_loss: 0.8940 - label_output_loss: 0.6908 - val_loss: 1.0363 - val_decoded_loss: 0.3471 - val_label_output_loss: 0.6893\n",
      "Epoch 6/1000\n",
      "346/346 [==============================] - 7s 22ms/step - loss: 1.5676 - decoded_loss: 0.8770 - label_output_loss: 0.6906 - val_loss: 1.0326 - val_decoded_loss: 0.3435 - val_label_output_loss: 0.6892\n",
      "Epoch 7/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.5619 - decoded_loss: 0.8714 - label_output_loss: 0.6905 - val_loss: 1.0261 - val_decoded_loss: 0.3372 - val_label_output_loss: 0.6889\n",
      "Epoch 8/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.5453 - decoded_loss: 0.8548 - label_output_loss: 0.6904 - val_loss: 1.0127 - val_decoded_loss: 0.3237 - val_label_output_loss: 0.6890\n",
      "Epoch 9/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.5415 - decoded_loss: 0.8512 - label_output_loss: 0.6903 - val_loss: 1.0021 - val_decoded_loss: 0.3134 - val_label_output_loss: 0.6887\n",
      "Epoch 10/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.5329 - decoded_loss: 0.8426 - label_output_loss: 0.6903 - val_loss: 1.0058 - val_decoded_loss: 0.3170 - val_label_output_loss: 0.6888\n",
      "Epoch 11/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.5268 - decoded_loss: 0.8367 - label_output_loss: 0.6902 - val_loss: 0.9963 - val_decoded_loss: 0.3077 - val_label_output_loss: 0.6886\n",
      "Epoch 12/1000\n",
      "346/346 [==============================] - 7s 22ms/step - loss: 1.5196 - decoded_loss: 0.8294 - label_output_loss: 0.6902 - val_loss: 0.9883 - val_decoded_loss: 0.2995 - val_label_output_loss: 0.6888\n",
      "Epoch 13/1000\n",
      "346/346 [==============================] - 8s 23ms/step - loss: 1.5187 - decoded_loss: 0.8287 - label_output_loss: 0.6901 - val_loss: 0.9887 - val_decoded_loss: 0.3000 - val_label_output_loss: 0.6887\n",
      "Epoch 14/1000\n",
      "346/346 [==============================] - 7s 22ms/step - loss: 1.5146 - decoded_loss: 0.8246 - label_output_loss: 0.6900 - val_loss: 0.9819 - val_decoded_loss: 0.2933 - val_label_output_loss: 0.6886\n",
      "Epoch 15/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.5137 - decoded_loss: 0.8237 - label_output_loss: 0.6900 - val_loss: 0.9783 - val_decoded_loss: 0.2898 - val_label_output_loss: 0.6886\n",
      "Epoch 16/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.5088 - decoded_loss: 0.8189 - label_output_loss: 0.6899 - val_loss: 0.9772 - val_decoded_loss: 0.2887 - val_label_output_loss: 0.6885\n",
      "Epoch 17/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.5047 - decoded_loss: 0.8148 - label_output_loss: 0.6899 - val_loss: 0.9955 - val_decoded_loss: 0.3068 - val_label_output_loss: 0.6886\n",
      "Epoch 18/1000\n",
      "346/346 [==============================] - 7s 22ms/step - loss: 1.5024 - decoded_loss: 0.8125 - label_output_loss: 0.6899 - val_loss: 0.9707 - val_decoded_loss: 0.2822 - val_label_output_loss: 0.6885\n",
      "Epoch 19/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.5062 - decoded_loss: 0.8163 - label_output_loss: 0.6898 - val_loss: 0.9748 - val_decoded_loss: 0.2863 - val_label_output_loss: 0.6885\n",
      "Epoch 20/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.5004 - decoded_loss: 0.8106 - label_output_loss: 0.6898 - val_loss: 0.9812 - val_decoded_loss: 0.2929 - val_label_output_loss: 0.6883\n",
      "Epoch 21/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.4968 - decoded_loss: 0.8069 - label_output_loss: 0.6898 - val_loss: 0.9656 - val_decoded_loss: 0.2772 - val_label_output_loss: 0.6884\n",
      "Epoch 22/1000\n",
      "346/346 [==============================] - 7s 22ms/step - loss: 1.5002 - decoded_loss: 0.8104 - label_output_loss: 0.6898 - val_loss: 0.9665 - val_decoded_loss: 0.2781 - val_label_output_loss: 0.6884\n",
      "Epoch 23/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.4930 - decoded_loss: 0.8033 - label_output_loss: 0.6897 - val_loss: 0.9617 - val_decoded_loss: 0.2735 - val_label_output_loss: 0.6882\n",
      "Epoch 24/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.4972 - decoded_loss: 0.8075 - label_output_loss: 0.6897 - val_loss: 0.9704 - val_decoded_loss: 0.2821 - val_label_output_loss: 0.6883\n",
      "Epoch 25/1000\n",
      "346/346 [==============================] - 7s 22ms/step - loss: 1.4942 - decoded_loss: 0.8045 - label_output_loss: 0.6897 - val_loss: 0.9576 - val_decoded_loss: 0.2692 - val_label_output_loss: 0.6884\n",
      "Epoch 26/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.4914 - decoded_loss: 0.8017 - label_output_loss: 0.6897 - val_loss: 0.9697 - val_decoded_loss: 0.2813 - val_label_output_loss: 0.6884\n",
      "Epoch 27/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.4900 - decoded_loss: 0.8004 - label_output_loss: 0.6897 - val_loss: 0.9632 - val_decoded_loss: 0.2749 - val_label_output_loss: 0.6883\n",
      "Epoch 28/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.4859 - decoded_loss: 0.7962 - label_output_loss: 0.6897 - val_loss: 0.9596 - val_decoded_loss: 0.2712 - val_label_output_loss: 0.6884\n",
      "Epoch 29/1000\n",
      "346/346 [==============================] - 7s 22ms/step - loss: 1.4928 - decoded_loss: 0.8031 - label_output_loss: 0.6896 - val_loss: 0.9633 - val_decoded_loss: 0.2749 - val_label_output_loss: 0.6884\n",
      "Epoch 30/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.4897 - decoded_loss: 0.8001 - label_output_loss: 0.6897 - val_loss: 0.9591 - val_decoded_loss: 0.2708 - val_label_output_loss: 0.6883\n",
      "Epoch 31/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.4926 - decoded_loss: 0.8029 - label_output_loss: 0.6896 - val_loss: 0.9588 - val_decoded_loss: 0.2705 - val_label_output_loss: 0.6884\n",
      "Epoch 32/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.4870 - decoded_loss: 0.7974 - label_output_loss: 0.6896 - val_loss: 0.9606 - val_decoded_loss: 0.2721 - val_label_output_loss: 0.6884\n",
      "Epoch 33/1000\n",
      "346/346 [==============================] - 7s 22ms/step - loss: 1.4858 - decoded_loss: 0.7961 - label_output_loss: 0.6897 - val_loss: 0.9582 - val_decoded_loss: 0.2700 - val_label_output_loss: 0.6882\n",
      "Epoch 34/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.4809 - decoded_loss: 0.7914 - label_output_loss: 0.6895 - val_loss: 0.9593 - val_decoded_loss: 0.2708 - val_label_output_loss: 0.6885\n",
      "Epoch 35/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.4870 - decoded_loss: 0.7974 - label_output_loss: 0.6896 - val_loss: 0.9534 - val_decoded_loss: 0.2652 - val_label_output_loss: 0.6883\n",
      "Epoch 36/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.4835 - decoded_loss: 0.7940 - label_output_loss: 0.6895 - val_loss: 0.9569 - val_decoded_loss: 0.2685 - val_label_output_loss: 0.6884\n",
      "Epoch 37/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.4809 - decoded_loss: 0.7913 - label_output_loss: 0.6896 - val_loss: 0.9522 - val_decoded_loss: 0.2639 - val_label_output_loss: 0.6883\n",
      "Epoch 38/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.4794 - decoded_loss: 0.7899 - label_output_loss: 0.6895 - val_loss: 0.9538 - val_decoded_loss: 0.2656 - val_label_output_loss: 0.6882\n",
      "Epoch 39/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.4828 - decoded_loss: 0.7933 - label_output_loss: 0.6895 - val_loss: 0.9621 - val_decoded_loss: 0.2737 - val_label_output_loss: 0.6884\n",
      "Epoch 40/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.4862 - decoded_loss: 0.7967 - label_output_loss: 0.6895 - val_loss: 0.9585 - val_decoded_loss: 0.2702 - val_label_output_loss: 0.6883\n",
      "Epoch 41/1000\n",
      "346/346 [==============================] - 7s 22ms/step - loss: 1.4826 - decoded_loss: 0.7931 - label_output_loss: 0.6895 - val_loss: 0.9510 - val_decoded_loss: 0.2627 - val_label_output_loss: 0.6883\n",
      "Epoch 42/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.4765 - decoded_loss: 0.7871 - label_output_loss: 0.6895 - val_loss: 0.9633 - val_decoded_loss: 0.2750 - val_label_output_loss: 0.6883\n",
      "Epoch 43/1000\n",
      "346/346 [==============================] - 7s 22ms/step - loss: 1.4768 - decoded_loss: 0.7873 - label_output_loss: 0.6895 - val_loss: 0.9510 - val_decoded_loss: 0.2628 - val_label_output_loss: 0.6882\n",
      "Epoch 44/1000\n",
      "346/346 [==============================] - 7s 22ms/step - loss: 1.4784 - decoded_loss: 0.7889 - label_output_loss: 0.6895 - val_loss: 0.9492 - val_decoded_loss: 0.2609 - val_label_output_loss: 0.6883\n",
      "Epoch 45/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.4781 - decoded_loss: 0.7887 - label_output_loss: 0.6895 - val_loss: 0.9475 - val_decoded_loss: 0.2591 - val_label_output_loss: 0.6884\n",
      "Epoch 46/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.4768 - decoded_loss: 0.7873 - label_output_loss: 0.6894 - val_loss: 0.9611 - val_decoded_loss: 0.2728 - val_label_output_loss: 0.6882\n",
      "Epoch 47/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.4842 - decoded_loss: 0.7948 - label_output_loss: 0.6894 - val_loss: 0.9595 - val_decoded_loss: 0.2714 - val_label_output_loss: 0.6881\n",
      "Epoch 48/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.4762 - decoded_loss: 0.7867 - label_output_loss: 0.6895 - val_loss: 0.9455 - val_decoded_loss: 0.2572 - val_label_output_loss: 0.6883\n",
      "Epoch 49/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.4725 - decoded_loss: 0.7831 - label_output_loss: 0.6894 - val_loss: 0.9596 - val_decoded_loss: 0.2714 - val_label_output_loss: 0.6883\n",
      "Epoch 50/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.4796 - decoded_loss: 0.7901 - label_output_loss: 0.6894 - val_loss: 0.9584 - val_decoded_loss: 0.2699 - val_label_output_loss: 0.6884\n",
      "Epoch 51/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.4800 - decoded_loss: 0.7906 - label_output_loss: 0.6894 - val_loss: 0.9475 - val_decoded_loss: 0.2592 - val_label_output_loss: 0.6883\n",
      "Epoch 52/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.4760 - decoded_loss: 0.7866 - label_output_loss: 0.6894 - val_loss: 0.9519 - val_decoded_loss: 0.2638 - val_label_output_loss: 0.6881\n",
      "Epoch 53/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.4748 - decoded_loss: 0.7853 - label_output_loss: 0.6894 - val_loss: 0.9487 - val_decoded_loss: 0.2604 - val_label_output_loss: 0.6882\n",
      "Epoch 54/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.4717 - decoded_loss: 0.7823 - label_output_loss: 0.6894 - val_loss: 0.9498 - val_decoded_loss: 0.2616 - val_label_output_loss: 0.6882\n",
      "Epoch 55/1000\n",
      "346/346 [==============================] - 7s 22ms/step - loss: 1.4756 - decoded_loss: 0.7862 - label_output_loss: 0.6894 - val_loss: 0.9466 - val_decoded_loss: 0.2584 - val_label_output_loss: 0.6881\n",
      "Epoch 56/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.4789 - decoded_loss: 0.7896 - label_output_loss: 0.6894 - val_loss: 0.9566 - val_decoded_loss: 0.2683 - val_label_output_loss: 0.6883\n",
      "Epoch 57/1000\n",
      "346/346 [==============================] - 7s 21ms/step - loss: 1.4696 - decoded_loss: 0.7803 - label_output_loss: 0.6893 - val_loss: 0.9528 - val_decoded_loss: 0.2645 - val_label_output_loss: 0.6883\n",
      "Epoch 58/1000\n",
      "346/346 [==============================] - 8s 22ms/step - loss: 1.4741 - decoded_loss: 0.7847 - label_output_loss: 0.6894 - val_loss: 0.9511 - val_decoded_loss: 0.2630 - val_label_output_loss: 0.6881\n"
     ]
    }
   ],
   "source": [
    "autoencoder, encoder = create_autoencoder(X.shape[-1],y.shape[-1],noise=0.1)\n",
    "if TRAINING:\n",
    "    autoencoder.fit(X,(X,y),\n",
    "                    epochs=1000,\n",
    "                    batch_size=4096, \n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[EarlyStopping('val_loss',patience=10,restore_best_weights=True)])\n",
    "    encoder.save_weights(f'./{CACHE_PATH}/encoder.hdf5')\n",
    "else:\n",
    "    encoder.load_weights(f'./{CACHE_PATH}/encoder.hdf5')\n",
    "encoder.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013593,
     "end_time": "2020-12-23T13:10:35.379172",
     "exception": false,
     "start_time": "2020-12-23T13:10:35.365579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Running CV\n",
    "Following [this notebook](https://www.kaggle.com/gogo827jz/jane-street-ffill-xgboost-purgedtimeseriescv) which use 5 PurgedGroupTimeSeriesSplit split on the dates in the training data. \n",
    "\n",
    "We add the locked encoder as the first layer of the MLP. This seems to help in speeding up the submission rather than first predicting using the encoder then using the MLP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013611,
     "end_time": "2020-12-23T13:10:35.406484",
     "exception": false,
     "start_time": "2020-12-23T13:10:35.392873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We use a Baysian Optimizer to find the optimal HPs for our model. 20 trials take about 2 hours on GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "papermill": {
     "duration": 0.052076,
     "end_time": "2020-12-23T13:08:03.254102",
     "exception": false,
     "start_time": "2020-12-23T13:08:03.202026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CVTuner(kt.engine.tuner.Tuner):\n",
    "    def run_trial(self, trial, X, y, splits, batch_size=32, epochs=1,callbacks=None):\n",
    "        val_losses = []\n",
    "        for train_indices, test_indices in splits:\n",
    "            X_train, X_test = [x[train_indices] for x in X], [x[test_indices] for x in X]\n",
    "            y_train, y_test = [a[train_indices] for a in y], [a[test_indices] for a in y]\n",
    "            if len(X_train) < 2:\n",
    "                X_train = X_train[0]\n",
    "                X_test = X_test[0]\n",
    "            if len(y_train) < 2:\n",
    "                y_train = y_train[0]\n",
    "                y_test = y_test[0]\n",
    "            \n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            hist = model.fit(X_train,y_train,\n",
    "                      validation_data=(X_test,y_test),\n",
    "                      epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                      callbacks=callbacks)\n",
    "            \n",
    "            val_losses.append([hist.history[k][-1] for k in hist.history])\n",
    "        val_losses = np.asarray(val_losses)\n",
    "        self.oracle.update_trial(trial.trial_id, {k:np.mean(val_losses[:,i]) for i,k in enumerate(hist.history.keys())})\n",
    "        self.save_model(trial.trial_id, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 1.987781,
     "end_time": "2020-12-23T13:10:37.408256",
     "exception": false,
     "start_time": "2020-12-23T13:10:35.420475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 03m 16s]\n",
      "val_auc: 0.542873427271843\n",
      "\n",
      "Best val_auc So Far: 0.542873427271843\n",
      "Total elapsed time: 00h 03m 16s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "fitting...\n",
      "Epoch 1/100\n",
      "48/48 [==============================] - 2s 38ms/step - loss: 0.7243 - auc: 0.5143 - val_loss: 0.6969 - val_auc: 0.5293\n",
      "Epoch 2/100\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.6954 - auc: 0.5281 - val_loss: 0.6920 - val_auc: 0.5356\n",
      "Epoch 3/100\n",
      "48/48 [==============================] - 2s 31ms/step - loss: 0.6917 - auc: 0.5374 - val_loss: 0.6911 - val_auc: 0.5386\n",
      "Epoch 4/100\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.6904 - auc: 0.5424 - val_loss: 0.6908 - val_auc: 0.5400\n",
      "Epoch 5/100\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.6895 - auc: 0.5458 - val_loss: 0.6910 - val_auc: 0.5388\n",
      "Epoch 6/100\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.6887 - auc: 0.5505 - val_loss: 0.6907 - val_auc: 0.5407\n",
      "Epoch 7/100\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.6886 - auc: 0.5510 - val_loss: 0.6910 - val_auc: 0.5394\n",
      "Epoch 8/100\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.6882 - auc: 0.5529 - val_loss: 0.6907 - val_auc: 0.5414\n",
      "Epoch 9/100\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.6878 - auc: 0.5554 - val_loss: 0.6909 - val_auc: 0.5404\n",
      "Epoch 10/100\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.6877 - auc: 0.5558 - val_loss: 0.6909 - val_auc: 0.5404\n",
      "Epoch 11/100\n",
      "48/48 [==============================] - 1s 28ms/step - loss: 0.6875 - auc: 0.5568 - val_loss: 0.6909 - val_auc: 0.5418\n",
      "Epoch 12/100\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.6873 - auc: 0.5572 - val_loss: 0.6910 - val_auc: 0.5402\n",
      "Epoch 13/100\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.6869 - auc: 0.5585 - val_loss: 0.6909 - val_auc: 0.5404\n",
      "Epoch 14/100\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.6868 - auc: 0.5593 - val_loss: 0.6911 - val_auc: 0.5404\n",
      "Epoch 15/100\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.6867 - auc: 0.5597 - val_loss: 0.6911 - val_auc: 0.5397\n",
      "Epoch 16/100\n",
      "48/48 [==============================] - 1s 29ms/step - loss: 0.6865 - auc: 0.5601 - val_loss: 0.6912 - val_auc: 0.5401\n",
      "Epoch 17/100\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 0.6864 - auc: 0.5610 - val_loss: 0.6910 - val_auc: 0.5409\n",
      "Epoch 18/100\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.6863 - auc: 0.5617 - val_loss: 0.6914 - val_auc: 0.5396\n",
      "Epoch 19/100\n",
      "48/48 [==============================] - 1s 31ms/step - loss: 0.6861 - auc: 0.5616 - val_loss: 0.6913 - val_auc: 0.5402\n",
      "Epoch 20/100\n",
      "48/48 [==============================] - 2s 32ms/step - loss: 0.6858 - auc: 0.5637 - val_loss: 0.6912 - val_auc: 0.5411\n",
      "Epoch 21/100\n",
      "48/48 [==============================] - 2s 33ms/step - loss: 0.6859 - auc: 0.5632 - val_loss: 0.6914 - val_auc: 0.5384\n",
      "Epoch 1/3\n",
      "71/71 [==============================] - 1s 18ms/step - loss: 0.6920\n",
      "Epoch 2/3\n",
      "71/71 [==============================] - 1s 18ms/step - loss: 0.6916\n",
      "Epoch 3/3\n",
      "71/71 [==============================] - 1s 18ms/step - loss: 0.6913\n",
      "Epoch 1/100\n",
      "118/118 [==============================] - 3s 26ms/step - loss: 0.7070 - auc: 0.5186 - val_loss: 0.6905 - val_auc: 0.5403\n",
      "Epoch 2/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.6915 - auc: 0.5351 - val_loss: 0.6901 - val_auc: 0.5441\n",
      "Epoch 3/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.6904 - auc: 0.5411 - val_loss: 0.6900 - val_auc: 0.5449\n",
      "Epoch 4/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6900 - auc: 0.5438 - val_loss: 0.6900 - val_auc: 0.5451\n",
      "Epoch 5/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6897 - auc: 0.5455 - val_loss: 0.6898 - val_auc: 0.5464\n",
      "Epoch 6/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.6895 - auc: 0.5469 - val_loss: 0.6899 - val_auc: 0.5458\n",
      "Epoch 7/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6893 - auc: 0.5478 - val_loss: 0.6898 - val_auc: 0.5457\n",
      "Epoch 8/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6891 - auc: 0.5489 - val_loss: 0.6899 - val_auc: 0.5451\n",
      "Epoch 9/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6890 - auc: 0.5495 - val_loss: 0.6898 - val_auc: 0.5461\n",
      "Epoch 10/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.6888 - auc: 0.5500 - val_loss: 0.6899 - val_auc: 0.5449\n",
      "Epoch 11/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6887 - auc: 0.5510 - val_loss: 0.6900 - val_auc: 0.5450\n",
      "Epoch 12/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.6886 - auc: 0.5512 - val_loss: 0.6898 - val_auc: 0.5465\n",
      "Epoch 13/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.6885 - auc: 0.5519 - val_loss: 0.6897 - val_auc: 0.5463\n",
      "Epoch 14/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.6884 - auc: 0.5522 - val_loss: 0.6899 - val_auc: 0.5455\n",
      "Epoch 15/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.6883 - auc: 0.5525 - val_loss: 0.6897 - val_auc: 0.5461\n",
      "Epoch 16/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.6882 - auc: 0.5534 - val_loss: 0.6900 - val_auc: 0.5454\n",
      "Epoch 17/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6881 - auc: 0.5539 - val_loss: 0.6899 - val_auc: 0.5454\n",
      "Epoch 18/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.6881 - auc: 0.5538 - val_loss: 0.6902 - val_auc: 0.5435\n",
      "Epoch 19/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6880 - auc: 0.5542 - val_loss: 0.6898 - val_auc: 0.5461\n",
      "Epoch 20/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.6878 - auc: 0.5548 - val_loss: 0.6899 - val_auc: 0.5455\n",
      "Epoch 21/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6878 - auc: 0.5548 - val_loss: 0.6899 - val_auc: 0.5464\n",
      "Epoch 22/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6877 - auc: 0.5554 - val_loss: 0.6898 - val_auc: 0.5466\n",
      "Epoch 23/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6877 - auc: 0.5553 - val_loss: 0.6900 - val_auc: 0.5461\n",
      "Epoch 24/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.6876 - auc: 0.5559 - val_loss: 0.6899 - val_auc: 0.5465\n",
      "Epoch 25/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6875 - auc: 0.5567 - val_loss: 0.6901 - val_auc: 0.5454\n",
      "Epoch 26/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6874 - auc: 0.5567 - val_loss: 0.6898 - val_auc: 0.5473\n",
      "Epoch 27/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.6874 - auc: 0.5571 - val_loss: 0.6899 - val_auc: 0.5453\n",
      "Epoch 28/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6874 - auc: 0.5566 - val_loss: 0.6900 - val_auc: 0.5457\n",
      "Epoch 29/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6872 - auc: 0.5578 - val_loss: 0.6900 - val_auc: 0.5462\n",
      "Epoch 30/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6871 - auc: 0.5581 - val_loss: 0.6901 - val_auc: 0.5451\n",
      "Epoch 31/100\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.6871 - auc: 0.5581 - val_loss: 0.6900 - val_auc: 0.5461\n",
      "Epoch 32/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6872 - auc: 0.5581 - val_loss: 0.6901 - val_auc: 0.5451\n",
      "Epoch 33/100\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.6870 - auc: 0.5585 - val_loss: 0.6900 - val_auc: 0.5462\n",
      "Epoch 34/100\n",
      "118/118 [==============================] - 3s 24ms/step - loss: 0.6870 - auc: 0.5586 - val_loss: 0.6899 - val_auc: 0.5465\n",
      "Epoch 35/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.6869 - auc: 0.5590 - val_loss: 0.6899 - val_auc: 0.5470\n",
      "Epoch 36/100\n",
      "118/118 [==============================] - 3s 25ms/step - loss: 0.6868 - auc: 0.5594 - val_loss: 0.6901 - val_auc: 0.5469\n",
      "Epoch 1/3\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.6906\n",
      "Epoch 2/3\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.6903\n",
      "Epoch 3/3\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.6902\n",
      "Epoch 1/100\n",
      "193/193 [==============================] - 5s 25ms/step - loss: 0.7020 - auc: 0.5236 - val_loss: 0.6912 - val_auc: 0.5338\n",
      "Epoch 2/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6905 - auc: 0.5406 - val_loss: 0.6911 - val_auc: 0.5353\n",
      "Epoch 3/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6900 - auc: 0.5443 - val_loss: 0.6911 - val_auc: 0.5348\n",
      "Epoch 4/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6898 - auc: 0.5452 - val_loss: 0.6911 - val_auc: 0.5354\n",
      "Epoch 5/100\n",
      "193/193 [==============================] - 4s 22ms/step - loss: 0.6896 - auc: 0.5468 - val_loss: 0.6911 - val_auc: 0.5361\n",
      "Epoch 6/100\n",
      "193/193 [==============================] - 4s 22ms/step - loss: 0.6895 - auc: 0.5471 - val_loss: 0.6913 - val_auc: 0.5349\n",
      "Epoch 7/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6894 - auc: 0.5480 - val_loss: 0.6912 - val_auc: 0.5363\n",
      "Epoch 8/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6893 - auc: 0.5484 - val_loss: 0.6910 - val_auc: 0.5358\n",
      "Epoch 9/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6891 - auc: 0.5489 - val_loss: 0.6910 - val_auc: 0.5364\n",
      "Epoch 10/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6890 - auc: 0.5499 - val_loss: 0.6910 - val_auc: 0.5370\n",
      "Epoch 11/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6889 - auc: 0.5503 - val_loss: 0.6911 - val_auc: 0.5364\n",
      "Epoch 12/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6889 - auc: 0.5504 - val_loss: 0.6911 - val_auc: 0.5358\n",
      "Epoch 13/100\n",
      "193/193 [==============================] - 4s 22ms/step - loss: 0.6887 - auc: 0.5512 - val_loss: 0.6909 - val_auc: 0.5384\n",
      "Epoch 14/100\n",
      "193/193 [==============================] - 5s 23ms/step - loss: 0.6887 - auc: 0.5511 - val_loss: 0.6909 - val_auc: 0.5375\n",
      "Epoch 15/100\n",
      "193/193 [==============================] - 4s 22ms/step - loss: 0.6887 - auc: 0.5516 - val_loss: 0.6910 - val_auc: 0.5372\n",
      "Epoch 16/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6886 - auc: 0.5519 - val_loss: 0.6909 - val_auc: 0.5374\n",
      "Epoch 17/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6884 - auc: 0.5526 - val_loss: 0.6910 - val_auc: 0.5378\n",
      "Epoch 18/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6884 - auc: 0.5526 - val_loss: 0.6912 - val_auc: 0.5360\n",
      "Epoch 19/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6884 - auc: 0.5531 - val_loss: 0.6911 - val_auc: 0.5376\n",
      "Epoch 20/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6883 - auc: 0.5530 - val_loss: 0.6910 - val_auc: 0.5381\n",
      "Epoch 21/100\n",
      "193/193 [==============================] - 4s 22ms/step - loss: 0.6882 - auc: 0.5539 - val_loss: 0.6909 - val_auc: 0.5375\n",
      "Epoch 22/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6882 - auc: 0.5536 - val_loss: 0.6909 - val_auc: 0.5386\n",
      "Epoch 23/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6881 - auc: 0.5539 - val_loss: 0.6909 - val_auc: 0.5382\n",
      "Epoch 24/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6881 - auc: 0.5540 - val_loss: 0.6909 - val_auc: 0.5386\n",
      "Epoch 25/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6880 - auc: 0.5549 - val_loss: 0.6909 - val_auc: 0.5391\n",
      "Epoch 26/100\n",
      "193/193 [==============================] - 4s 22ms/step - loss: 0.6880 - auc: 0.5546 - val_loss: 0.6908 - val_auc: 0.5386\n",
      "Epoch 27/100\n",
      "193/193 [==============================] - 4s 22ms/step - loss: 0.6879 - auc: 0.5550 - val_loss: 0.6911 - val_auc: 0.5378\n",
      "Epoch 28/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6879 - auc: 0.5550 - val_loss: 0.6912 - val_auc: 0.5384\n",
      "Epoch 29/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6878 - auc: 0.5555 - val_loss: 0.6911 - val_auc: 0.5379\n",
      "Epoch 30/100\n",
      "193/193 [==============================] - 5s 24ms/step - loss: 0.6878 - auc: 0.5555 - val_loss: 0.6910 - val_auc: 0.5388\n",
      "Epoch 31/100\n",
      "193/193 [==============================] - 5s 24ms/step - loss: 0.6877 - auc: 0.5560 - val_loss: 0.6910 - val_auc: 0.5386\n",
      "Epoch 32/100\n",
      "193/193 [==============================] - 5s 24ms/step - loss: 0.6877 - auc: 0.5559 - val_loss: 0.6909 - val_auc: 0.5383\n",
      "Epoch 33/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6876 - auc: 0.5564 - val_loss: 0.6907 - val_auc: 0.5390\n",
      "Epoch 34/100\n",
      "193/193 [==============================] - 4s 23ms/step - loss: 0.6876 - auc: 0.5565 - val_loss: 0.6908 - val_auc: 0.5390\n",
      "Epoch 35/100\n",
      "193/193 [==============================] - 5s 23ms/step - loss: 0.6875 - auc: 0.5571 - val_loss: 0.6912 - val_auc: 0.5371\n",
      "Epoch 1/3\n",
      "83/83 [==============================] - 2s 18ms/step - loss: 0.6916\n",
      "Epoch 2/3\n",
      "83/83 [==============================] - 2s 19ms/step - loss: 0.6913\n",
      "Epoch 3/3\n",
      "83/83 [==============================] - 2s 19ms/step - loss: 0.6911\n",
      "Epoch 1/100\n",
      "276/276 [==============================] - 6s 23ms/step - loss: 0.6987 - auc: 0.5263 - val_loss: 0.6898 - val_auc: 0.5458\n",
      "Epoch 2/100\n",
      "276/276 [==============================] - 6s 21ms/step - loss: 0.6904 - auc: 0.5410 - val_loss: 0.6898 - val_auc: 0.5450\n",
      "Epoch 3/100\n",
      "276/276 [==============================] - 6s 23ms/step - loss: 0.6901 - auc: 0.5430 - val_loss: 0.6895 - val_auc: 0.5471\n",
      "Epoch 4/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6899 - auc: 0.5444 - val_loss: 0.6892 - val_auc: 0.5491\n",
      "Epoch 5/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6898 - auc: 0.5453 - val_loss: 0.6891 - val_auc: 0.5498\n",
      "Epoch 6/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6896 - auc: 0.5464 - val_loss: 0.6892 - val_auc: 0.5492\n",
      "Epoch 7/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6895 - auc: 0.5465 - val_loss: 0.6889 - val_auc: 0.5508\n",
      "Epoch 8/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6894 - auc: 0.5476 - val_loss: 0.6892 - val_auc: 0.5488\n",
      "Epoch 9/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6893 - auc: 0.5478 - val_loss: 0.6889 - val_auc: 0.5511\n",
      "Epoch 10/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6892 - auc: 0.5483 - val_loss: 0.6889 - val_auc: 0.5514\n",
      "Epoch 11/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6891 - auc: 0.5489 - val_loss: 0.6889 - val_auc: 0.5512\n",
      "Epoch 12/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6891 - auc: 0.5490 - val_loss: 0.6889 - val_auc: 0.5517\n",
      "Epoch 13/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6890 - auc: 0.5496 - val_loss: 0.6890 - val_auc: 0.5510\n",
      "Epoch 14/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6890 - auc: 0.5498 - val_loss: 0.6888 - val_auc: 0.5520\n",
      "Epoch 15/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6889 - auc: 0.5500 - val_loss: 0.6886 - val_auc: 0.5526\n",
      "Epoch 16/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6888 - auc: 0.5503 - val_loss: 0.6887 - val_auc: 0.5531\n",
      "Epoch 17/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6888 - auc: 0.5506 - val_loss: 0.6889 - val_auc: 0.5519\n",
      "Epoch 18/100\n",
      "276/276 [==============================] - 6s 23ms/step - loss: 0.6887 - auc: 0.5508 - val_loss: 0.6887 - val_auc: 0.5522\n",
      "Epoch 19/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6887 - auc: 0.5510 - val_loss: 0.6888 - val_auc: 0.5531\n",
      "Epoch 20/100\n",
      "276/276 [==============================] - 6s 23ms/step - loss: 0.6886 - auc: 0.5514 - val_loss: 0.6888 - val_auc: 0.5522\n",
      "Epoch 21/100\n",
      "276/276 [==============================] - 6s 21ms/step - loss: 0.6885 - auc: 0.5518 - val_loss: 0.6886 - val_auc: 0.5528\n",
      "Epoch 22/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6885 - auc: 0.5519 - val_loss: 0.6887 - val_auc: 0.5524\n",
      "Epoch 23/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6885 - auc: 0.5521 - val_loss: 0.6887 - val_auc: 0.5527\n",
      "Epoch 24/100\n",
      "276/276 [==============================] - 6s 21ms/step - loss: 0.6885 - auc: 0.5523 - val_loss: 0.6887 - val_auc: 0.5519\n",
      "Epoch 25/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6884 - auc: 0.5526 - val_loss: 0.6885 - val_auc: 0.5535\n",
      "Epoch 26/100\n",
      "276/276 [==============================] - 6s 21ms/step - loss: 0.6883 - auc: 0.5529 - val_loss: 0.6889 - val_auc: 0.5516\n",
      "Epoch 27/100\n",
      "276/276 [==============================] - 6s 23ms/step - loss: 0.6883 - auc: 0.5529 - val_loss: 0.6884 - val_auc: 0.5542\n",
      "Epoch 28/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6882 - auc: 0.5537 - val_loss: 0.6885 - val_auc: 0.5537\n",
      "Epoch 29/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6882 - auc: 0.5531 - val_loss: 0.6886 - val_auc: 0.5532\n",
      "Epoch 30/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6882 - auc: 0.5537 - val_loss: 0.6886 - val_auc: 0.5528\n",
      "Epoch 31/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6882 - auc: 0.5537 - val_loss: 0.6887 - val_auc: 0.5525\n",
      "Epoch 32/100\n",
      "276/276 [==============================] - 6s 21ms/step - loss: 0.6881 - auc: 0.5540 - val_loss: 0.6886 - val_auc: 0.5526\n",
      "Epoch 33/100\n",
      "276/276 [==============================] - 6s 21ms/step - loss: 0.6881 - auc: 0.5539 - val_loss: 0.6887 - val_auc: 0.5526\n",
      "Epoch 34/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6881 - auc: 0.5543 - val_loss: 0.6885 - val_auc: 0.5535\n",
      "Epoch 35/100\n",
      "276/276 [==============================] - 6s 20ms/step - loss: 0.6880 - auc: 0.5547 - val_loss: 0.6886 - val_auc: 0.5535\n",
      "Epoch 36/100\n",
      "276/276 [==============================] - 6s 21ms/step - loss: 0.6879 - auc: 0.5549 - val_loss: 0.6886 - val_auc: 0.5538\n",
      "Epoch 37/100\n",
      "276/276 [==============================] - 6s 22ms/step - loss: 0.6879 - auc: 0.5549 - val_loss: 0.6886 - val_auc: 0.5539\n",
      "Epoch 1/3\n",
      "90/90 [==============================] - 2s 19ms/step - loss: 0.6887\n",
      "Epoch 2/3\n",
      "90/90 [==============================] - 2s 19ms/step - loss: 0.6887\n",
      "Epoch 3/3\n",
      "90/90 [==============================] - 2s 19ms/step - loss: 0.6884\n",
      "Results summary\n",
      "Results in ./untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='val_auc', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "init_dropout: 0.032877123725517154\n",
      "num_layers: 1\n",
      "num_units_{i}: 148\n",
      "dropout_0: 0.453656032732682\n",
      "lr: 0.0022385467473166444\n",
      "label_smoothing: 0.06907411490337519\n",
      "Score: 0.542873427271843\n"
     ]
    }
   ],
   "source": [
    "model_fn = lambda hp: create_model(hp,X.shape[-1],y.shape[-1],encoder)\n",
    "\n",
    "tuner = CVTuner(\n",
    "        hypermodel=model_fn,\n",
    "        oracle=kt.oracles.BayesianOptimization(\n",
    "            objective= kt.Objective('val_auc', direction='max'),\n",
    "            num_initial_points=4,\n",
    "            max_trials=1))\n",
    "\n",
    "if TRAINING:\n",
    "    print('searching...')\n",
    "    gkf = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap=20)\n",
    "    splits = list(gkf.split(y, groups=date_index))\n",
    "    tuner.search((X,),\n",
    "                 (y,),\n",
    "                 splits=splits,\n",
    "                 batch_size=4096,\n",
    "                 epochs=100,\n",
    "                 callbacks=[EarlyStopping('val_auc', mode='max',patience=3)])\n",
    "    hp = tuner.get_best_hyperparameters(1)[0]\n",
    "    pd.to_pickle(hp,f'./{CACHE_PATH}/best_hp_{SEED}.pkl')\n",
    "    \n",
    "    print('fitting...')\n",
    "    for fold, (train_indices, test_indices) in enumerate(splits):\n",
    "        model = model_fn(hp)\n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "        model.fit(X_train, y_train,\n",
    "                  validation_data=(X_test,y_test),\n",
    "                  epochs=100,\n",
    "                  batch_size=4096,\n",
    "                  callbacks=[EarlyStopping('val_auc',mode='max',patience=10,restore_best_weights=True)])\n",
    "        model.save_weights(f'./{CACHE_PATH}/model_{SEED}_{fold}.hdf5')\n",
    "        model.compile(Adam(hp.get('lr')/100), loss='binary_crossentropy')\n",
    "        model.fit(X_test, y_test, epochs=3, batch_size=4096)\n",
    "        model.save_weights(f'./{CACHE_PATH}/model_{SEED}_{fold}_finetune.hdf5')\n",
    "    tuner.results_summary()\n",
    "    \n",
    "else:\n",
    "    models = []\n",
    "    hp = pd.read_pickle(f'./{CACHE_PATH}/best_hp_{SEED}.pkl')\n",
    "    for f in range(FOLDS):\n",
    "        model = model_fn(hp)\n",
    "        if USE_FINETUNE:\n",
    "            model.load_weights(f'./{CACHE_PATH}/model_{SEED}_{f}_finetune.hdf5')\n",
    "        else:\n",
    "            model.load_weights(f'./{CACHE_PATH}/model_{SEED}_{f}.hdf5')\n",
    "        models.append(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014766,
     "end_time": "2020-12-23T13:10:37.437317",
     "exception": false,
     "start_time": "2020-12-23T13:10:37.422551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'janestreet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5b570a28f65a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparentdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjanestreet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'janestreet'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "# for importing janestreet locally\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir) \n",
    "\n",
    "import janestreet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "papermill": {
     "duration": 334.468401,
     "end_time": "2020-12-23T13:16:11.919589",
     "exception": false,
     "start_time": "2020-12-23T13:10:37.451188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not TRAINING:\n",
    "    f = np.median\n",
    "    models = models[-2:]\n",
    "    env = janestreet.make_env()\n",
    "    th = 0.5\n",
    "    for (test_df, pred_df) in tqdm(env.iter_test()):\n",
    "        if test_df['weight'].item() > 0:\n",
    "            x_tt = test_df.loc[:, features].values\n",
    "            if np.isnan(x_tt[:, 1:].sum()):\n",
    "                x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n",
    "            pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\n",
    "            pred = f(pred)\n",
    "            pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n",
    "        else:\n",
    "            pred_df.action = 0\n",
    "        env.predict(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.976926,
     "end_time": "2020-12-23T13:16:13.872445",
     "exception": false,
     "start_time": "2020-12-23T13:16:12.895519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
